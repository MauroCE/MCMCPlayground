{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT\n",
    "import numpy as np                             # for fast array manipulation\n",
    "import torch                                   # Pytorch\n",
    "import torchvision                             # contains image datasets and many functions to manipulate images\n",
    "import torchvision.transforms as transforms    # to normalize, scale etc the dataset\n",
    "from torch.utils.data import DataLoader        # to load data into batches (for SGD)\n",
    "from torchvision.utils import make_grid        # Plotting. Makes a grid of tensors\n",
    "from torchvision.datasets import MNIST         # the classic handwritten digits dataset\n",
    "import matplotlib.pyplot as plt                # to plot our images\n",
    "import torch.nn as nn                          # Class that implements a model (such as a Neural Network)\n",
    "import torch.nn.functional as F                # contains activation functions, sampling layers and more \"functional\" stuff\n",
    "import torch.optim as optim                    # For optimization routines such as SGD, ADAM, ADAGRAD, etc\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Settings\n",
    "batch_size = 100      # How many images to use for a SGD update\n",
    "L = 1                 # Samples per data point. See section \"Likelihood Lower Bound\".\n",
    "e_hidden = 500        # Number of hidden units in the encoder. Chosen based on AEVB paper page 7, section \"Marginal Likelihood\"\n",
    "d_hidden = 500        # Number of hidden units in the decoder. Chosen based on AEVB paper page 7, section \"Marginal Likelihood\"\n",
    "latent_dim = 2        # Chosen based on AEVB paper, page 7, section \"Marginal Likelihood\"\n",
    "learning_rate = 0.001 # For SGD\n",
    "weight_decay = 1e-5   # For SGD\n",
    "epochs = 30        # Number of sweeps through the whole dataset, also called epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "t = transforms.Compose([\n",
    "                        transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Use transformation for both training and test set\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=t)\n",
    "testset  = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=t)\n",
    "\n",
    "# Load train and test set\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader  = DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder, Decoder and VAE\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "    Encoder Network. Must inherit from `nn.Module` provided by Pytorch. We only need to define 2 things:\n",
    "    \n",
    "    1) The components of the network (layers, activation functions, etc). This is done in __init__().\n",
    "    2) How the network uses such components to transform the network input into an output. This is done in a method called `forward()`.\n",
    "    \"\"\"\n",
    "    super(Encoder, self).__init__()\n",
    "    # Define Fully-Connected FeedForward Connections\n",
    "    self.hidden = nn.Linear(in_features=28*28, out_features=e_hidden)\n",
    "    # We need two separate layers. One is used for mu one is used for logvar.\n",
    "    self.mu_layer     = nn.Linear(in_features=e_hidden, out_features=latent_dim)\n",
    "    self.logvar_layer = nn.Linear(in_features=e_hidden, out_features=latent_dim)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    \"\"\"Defines how the network transforms the input x into an encoded representation.\"\"\"\n",
    "    # Pass input through the first set of connections\n",
    "    x = F.relu(self.hidden(x))\n",
    "    # Now pass it to one set of connections to get mu, and to another set of connections \n",
    "    # to get logvar\n",
    "    return self.mu_layer(x), self.logvar_layer(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "    Decoder Network. Works similarly to the encoder, except it takes an input from the latent space\n",
    "    and then outputs an image.\n",
    "    \"\"\"\n",
    "    super(Decoder, self).__init__()\n",
    "    # Define Fully-Connected FeedForward Connections\n",
    "    self.hidden = nn.Linear(in_features=latent_dim, out_features=d_hidden)\n",
    "    # Second set of FC connections. Here we only want one output\n",
    "    self.output_layer = nn.Linear(in_features=d_hidden, out_features=28*28)\n",
    "\n",
    "  def forward(self, z):\n",
    "    \"\"\"Defines how the network transforms the latent input z into a flatten image.\"\"\"\n",
    "    # Notice that we use a sigmoid function at the end to restrict output values between \n",
    "    # 0 and 1 so they can be interpreted as probabilities (?)\n",
    "    z = F.relu(self.hidden(z))\n",
    "    return torch.sigmoid(self.output_layer(z))\n",
    "\n",
    "class VAE(nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"Puts together Encoder & Decoder with the reparametrization trick.\"\"\"\n",
    "    super(VAE, self).__init__()\n",
    "    self.encoder = Encoder()\n",
    "    self.decoder = Decoder()\n",
    "\n",
    "  def sample_latent(self, mu, logvar):\n",
    "    if self.training:\n",
    "      # Get standard normal in the shape of mu\n",
    "      eps = torch.randn_like(mu)\n",
    "      # Use logarithmic properties to transform logvar to std. Then multiply\n",
    "      # and sum by latent mu\n",
    "      return eps.mul(torch.exp(0.5*logvar)).add_(mu)\n",
    "    else:   # This is used when testing \n",
    "      return mu    \n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"Transforms image into latent and then back to its reconstruction.\"\"\"\n",
    "    # Feed image to encoder. Obtain mean and logvar for the latent space\n",
    "    latent_mu, latent_logvar = self.encoder(x.view(-1, 28*28))\n",
    "    # Sample from the latent space with the give mean and variance using the \n",
    "    # reparametrization trick\n",
    "    z = self.sample_latent(latent_mu, latent_logvar)\n",
    "    # Decode the latent representation\n",
    "    return self.decoder(z), latent_mu, latent_logvar   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "def vae_loss(image, reconstruction, mu, logvar):\n",
    "  \"\"\"Loss for the Variational AutoEncoder.\"\"\"\n",
    "  # Compute the binary_crossentropy.\n",
    "  # Notice that we reshape them because in practice we don't receive just 1 image and 1 reconstruction, but we receive a whole batch!\n",
    "  reconstruction_loss = F.binary_cross_entropy(input=reconstruction.view(-1, 28*28), target=image.view(-1, 28*28), reduction='sum')\n",
    "  # Compute KL divergence using formula (closed-form)\n",
    "  kl = 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "  return reconstruction_loss - kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 30] average reconstruction error: 18412.400981\n",
      "Epoch [2 / 30] average reconstruction error: 16538.637767\n",
      "Epoch [3 / 30] average reconstruction error: 16174.702072\n",
      "Epoch [4 / 30] average reconstruction error: 15911.874202\n",
      "Epoch [5 / 30] average reconstruction error: 15736.603620\n",
      "Epoch [6 / 30] average reconstruction error: 15605.961963\n",
      "Epoch [7 / 30] average reconstruction error: 15505.511729\n",
      "Epoch [8 / 30] average reconstruction error: 15427.356743\n",
      "Epoch [9 / 30] average reconstruction error: 15358.654191\n",
      "Epoch [10 / 30] average reconstruction error: 15300.298895\n",
      "Epoch [11 / 30] average reconstruction error: 15242.972747\n",
      "Epoch [12 / 30] average reconstruction error: 15176.580259\n",
      "Epoch [13 / 30] average reconstruction error: 15128.440575\n",
      "Epoch [14 / 30] average reconstruction error: 15082.925669\n",
      "Epoch [15 / 30] average reconstruction error: 15046.687858\n",
      "Epoch [16 / 30] average reconstruction error: 15002.148455\n",
      "Epoch [17 / 30] average reconstruction error: 14972.418055\n",
      "Epoch [18 / 30] average reconstruction error: 14942.431654\n",
      "Epoch [19 / 30] average reconstruction error: 14911.648255\n",
      "Epoch [20 / 30] average reconstruction error: 14884.237837\n",
      "Epoch [21 / 30] average reconstruction error: 14858.303838\n",
      "Epoch [22 / 30] average reconstruction error: 14835.158442\n",
      "Epoch [23 / 30] average reconstruction error: 14806.462936\n",
      "Epoch [24 / 30] average reconstruction error: 14784.107848\n",
      "Epoch [25 / 30] average reconstruction error: 14770.773724\n",
      "Epoch [26 / 30] average reconstruction error: 14744.497812\n",
      "Epoch [27 / 30] average reconstruction error: 14732.567741\n",
      "Epoch [28 / 30] average reconstruction error: 14712.161323\n",
      "Epoch [29 / 30] average reconstruction error: 14695.636953\n",
      "Epoch [30 / 30] average reconstruction error: 14676.976258\n"
     ]
    }
   ],
   "source": [
    "# Instantiate VAE\n",
    "vae = VAE()\n",
    "\n",
    "# Pass VAE to the device (GPU or CPU)\n",
    "vae = vae.to(device)\n",
    "\n",
    "# Use Stochastic Gradient Descent as optimizer\n",
    "optimizer = optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Set VAE to training mode\n",
    "vae.train()\n",
    "\n",
    "# Store all losses here\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  # Put a zero into losses. This is where we will cumulatively sum all the losses\n",
    "  # from each batch. After all batches are done, we will divide by the number of batches\n",
    "  # to obtain the average loss per batch\n",
    "  losses.append(0)\n",
    "\n",
    "  # To compute the number of batches (since it varies depending on dataset size)\n",
    "  # update a counter variable\n",
    "  number_of_batches = 0\n",
    "\n",
    "  # Grab the batch, we are only interested in images not on their labels\n",
    "  for images, _ in trainloader:\n",
    "    # Store image batch to device\n",
    "    images = images.to(device)\n",
    "\n",
    "    # Set previous gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Feed images through the VAE to obtain their reconstruction\n",
    "    reconstructions, latent_mu, latent_logvar = vae(images)\n",
    "\n",
    "    # Compare reconstruction and images via the loss function\n",
    "    loss = vae_loss(images, reconstructions, latent_mu, latent_logvar)\n",
    "\n",
    "    # Backpropagate the loss. Before doing that, make sure all previously stored gradients are zero (e.g. from previous iterations)\n",
    "    loss.backward()\n",
    "\n",
    "    # Use the accumulated gradients to do a step in the right direction\n",
    "    optimizer.step()\n",
    "\n",
    "    # Add loss to the cumulative sum\n",
    "    losses[-1] += loss.item()        #.item() grabs the value in a tensor with only 1 value\n",
    "    number_of_batches += 1\n",
    "  \n",
    "  # At the end of all batches divide the total loss for this epoch by the number of \n",
    "  # batches to obtain an average loss per batch\n",
    "  losses[-1] /= number_of_batches\n",
    "  print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, epochs, losses[-1]))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE with Normalizing Flows (Planar Flows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory of Vanilla VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a vanilla VAE we feed $x$ into an encoder neural network and obtain $(\\mu, \\log \\sigma)$. These are the parameters of our approximate distribution \n",
    "\n",
    "$$\n",
    "q_{\\phi}(z \\mid x) = \\mathcal{N}(z \\mid \\mu_{\\phi}(x), \\sigma^2_{\\phi}(x) I)\n",
    "$$\n",
    "\n",
    "We then get a sample $z \\sim q_{\\phi}(z \\mid x)$ by using the reparametrization trick $z = \\mu + \\sigma \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, 1)$. We then use the following objective function\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\phi, \\theta}(x) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] - \\text{KL}(q_{\\phi}(z \\mid x) \\parallel p(z))\n",
    "$$\n",
    "\n",
    "where we compute the KL divergence in closed form using \n",
    "\n",
    "$$\n",
    "\\text{KL}(q_{\\phi}(z \\mid x) \\parallel p(z)) = -\\frac{1}{2}\\sum_{j=1}^{\\text{dim}(z)} \\left(1 + \\log\\sigma_j^2 - \\mu_j^2 - \\sigma_j^2\\right)\n",
    "$$\n",
    "\n",
    "and we can compute the reconstruction error in two simple cases: Bernoulli and Normal. In the Bernoulli case (e.g. when working with images) we have\n",
    "\n",
    "$$\n",
    "p_{\\theta}(x \\mid z) = \\prod_{i=1}^{\\text{dim}(x)} p_i(z)^{x_i}(1 - p_i(z))^{1 - x_i}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "p = (p_1(z), \\ldots, p_{\\text{dim}(x)}(z))^\\top \n",
    "$$\n",
    "is the output of the decoder network: $z \\longrightarrow $ Decoder $ \\longrightarrow p \\in [0, 1]^{\\text{dim}(x)}$. This means that we can write the reconstruction error as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)]\n",
    "    &=  \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[\\log \\prod_{i=1}^{\\text{dim}(x)} p_i(z)^{x_i}(1 - p_i(z))^{1-  x_i}\\right] \\\\\n",
    "    &= \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[\\sum_{i=1}^{\\text{dim}(x)} x_i \\log p_i(z) + (1 - x_i) \\log(1 - p_i(z))\\right] \\\\\n",
    "    &\\approx \\sum_{j=1}^{n_{z}}\\sum_{i=1}^{\\text{dim}(x)} x_i \\log p_i(z) + (1 - x_i) \\log(1 - p_i(z)) \\qquad z^{(j)} \\sim q_{\\phi}(z \\mid x)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $n_z$ is the number of samples that we sample from $q_{\\phi}(z \\mid x)$. Usually, we simply set $n_z = 1$, that is we only sample one latent variable for each datapoint. This leads to the following objective function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\phi, \\theta}(x) = \\sum_{i=1}^{\\text{dim}(x)} x_i \\log p_i(z^{(j)}) + (1 - x_i) \\log(1 - p_i(z^{(j)})) +\\frac{1}{2}\\sum_{j=1}^{\\text{dim}(z)} \\left(1 + \\log\\sigma_j^2 - \\mu_j^2 - \\sigma_j^2\\right)\n",
    "$$\n",
    "\n",
    "which was coded as follow:\n",
    "\n",
    "```python\n",
    "def vae_loss(image, reconstruction, mu, logvar):\n",
    "  \"\"\"Loss for the Variational AutoEncoder.\"\"\"\n",
    "  # Compute the binary_crossentropy.\n",
    "  recon_loss = F.binary_cross_entropy(\n",
    "      input=reconstruction.view(-1, 28*28),    # input is p(z) (the mean reconstruction)\n",
    "      target=image.view(-1, 28*28),            # target is x   (the true image)\n",
    "      reduction='sum'                          \n",
    "  )\n",
    "  # Compute KL divergence using formula (closed-form)\n",
    "  kl = 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "  return reconstruction_loss - kl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory of Normalizing Flows VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we not only want our Encoder to output $(\\mu, \\log \\sigma)$ to shift and scale our standard normal $\\epsilon \\sim \\mathcal{N}(0, 1)$. We also want to feed it through a series of transformations depending on some parameters $\\lambda$. In particular, we would like our Encoder to work as follows:\n",
    "\n",
    "$$\n",
    "x \\longrightarrow \\text{Encoder} \\longrightarrow (\\mu, \\log\\sigma, \\lambda)\n",
    "$$\n",
    "\n",
    "then we would use $(\\mu, \\log\\sigma)$ to compute $z_0$ using the reparametrization trick\n",
    "\n",
    "$$\n",
    "z_0 = \\mu + \\sigma \\epsilon \\qquad \\epsilon \\sim \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "and finally, we would feed $z_0$ into a series of transformation (with parameters $\\lambda$) to reach the final $z_K$:\n",
    "\n",
    "$$\n",
    "z_K = f_K \\circ \\ldots \\circ f_2 \\circ f_1(z_0)\n",
    "$$\n",
    "\n",
    "This means that our approximating distribution would not be \n",
    "\n",
    "$$\n",
    "q_{\\phi}(z \\mid x) = \\mathcal{N}(z \\mid \\mu_{\\phi}(x), \\sigma^2_{\\phi}(x) I)\n",
    "$$\n",
    "\n",
    "anymore but rather, we would have $q_0(z_0)=\\mathcal{N}(z_0 \\mid \\mu_{\\phi}(x), \\sigma_{\\phi}^2(x) I)$ and \n",
    "(using the change of variables formula)\n",
    "\n",
    "$$\n",
    "\\ln q_{\\phi}(z \\mid x) = \\ln q_K(z_K) = \\ln q_0(z_0) - \\sum_{k=1}^K \\ln \\left|\\text{det}\\frac{\\partial f_k}{\\partial z_{k-1}}\\right|.\n",
    "$$\n",
    "Thanks to the law of the uncounscious statistician we now know that \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{\\phi, \\theta}(x) \n",
    "&= \\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] - \\text{KL}(q_{\\phi}(z \\mid x) \\parallel p(z)) \\\\\n",
    "&= \\mathbb{E}_{q_K(z_K)}[\\log p_{\\theta}(x \\mid z_K)] - \\mathbb{E}_{q_K(z_K)}[\\log q_K(z_K) - \\log p(z_K)] \\\\\n",
    "&= \\mathbb{E}_{q_0(z_0)}[\\log p_{\\theta}(x \\mid f_K \\circ \\ldots \\circ f_2 \\circ f_1(z_0))] - \\mathbb{E}_{q_0(z_0)}[\\log q_K(f_K \\circ \\ldots \\circ f_2 \\circ f_1(z_0)) - \\log p(f_K \\circ \\ldots \\circ f_2 \\circ f_1(z_0))]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "as usual, we can approximate this with Monte Carlo by drawing samples $z_0 \\sim q_0(z_0) = N(\\mu, \\sigma^2 I)$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\phi, \\theta}(x) \\approx \\sum_{j=1}^{n_z}\\log p_{\\theta}(x \\mid f_K \\circ \\ldots \\circ f_2 \\circ f_1(z_0^j)) - \\sum_{j=1}^{n_z}\\log q_K(f_K \\circ \\ldots \\circ f_2 \\circ f_1(z_0^j)) - \\log p(f_K \\circ \\ldots \\circ f_2 \\circ f_1(z_0^j))\n",
    "$$\n",
    "\n",
    "Again, in practice we use only one sample\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{\\phi, \\theta}(x) \n",
    "&\\approx \\left[\\sum_{i=1}^{\\text{dim}(x)} x_i \\log p_i(z_K) + (1 - x_i) \\log(1 - p_i(z_K))\\right] - \\log q_K(z_K) + \\log p(z_K) \\\\\n",
    "&= \\text{BCE}(X, z_K) - (\\log q_0(z_0) + \\text{LADJ}) + \\log p(z_K) \\\\\n",
    "&= \\text{BCE}(X, z_K) - \\log q_0(z_0) - \\text{LADJ} + \\log p(z_K)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In our case we use the transformation\n",
    "\n",
    "$$\n",
    "f(z) = u h(w^\\top z + b) \\qquad u, w \\in \\mathbb{R}^{\\text{dim}(z)\\times 1} \\qquad b\\in\\mathbb{R} \\qquad h(\\cdot)  =\\tanh(\\cdot)\n",
    "$$\n",
    "\n",
    "It's Log-Absolute-Determinant-Jacobian (LADJ) is given by\n",
    "\n",
    "$$\n",
    "\\log \\left|\\text{det}\\frac{\\partial f}{\\partial z}\\right| = \\log |1 + u^\\top h'(w^\\top z + b)w|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(z) = z + u h(w^\\top z + b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "| \\text{det}\\frac{\\partial f}{\\partial z}| = |1 + u^\\top \\psi(z)| \\qquad \\psi(z) = h'(w^\\top z + b)w\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder, Decoder and VAE\n",
    "class EncoderPF(nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "    Encoder Network. Must inherit from `nn.Module` provided by Pytorch. We only need to define 2 things:\n",
    "    \n",
    "    1) The components of the network (layers, activation functions, etc). This is done in __init__().\n",
    "    2) How the network uses such components to transform the network input into an output. This is done in a method called `forward()`.\n",
    "    \"\"\"\n",
    "    super(Encoder, self).__init__()\n",
    "    # Define Fully-Connected FeedForward Connections\n",
    "    self.hidden = nn.Linear(in_features=28*28, out_features=e_hidden)\n",
    "    # Parameters for q_0\n",
    "    self.mu_layer     = nn.Linear(in_features=e_hidden, out_features=latent_dim)\n",
    "    self.logvar_layer = nn.Linear(in_features=e_hidden, out_features=latent_dim)\n",
    "    # Parameters for Normalizing Flow\n",
    "    self.w = nn.Linear(in_features=e_hidden, out_features=latent_dim)\n",
    "    self.u = nn.Linear(in_features=e_hidden, out_features=latent_dim)\n",
    "    self.b = nn.Linear(in_features=e_hidden, out_features=latent_dim)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    \"\"\"Defines how the network transforms the input x into an encoded representation.\"\"\"\n",
    "    # Pass input through the first set of connections\n",
    "    h = F.relu(self.hidden(x))\n",
    "    # Now pass it to one set of connections to get mu, and to another set of connections \n",
    "    # to get logvar\n",
    "    return self.mu_layer(h), self.logvar_layer(h), self.w(h), self.u(h), self.b(h)\n",
    "\n",
    "class DecoderPF(nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "    Decoder Network. Works similarly to the encoder, except it takes an input from the latent space\n",
    "    and then outputs an image.\n",
    "    \"\"\"\n",
    "    super(Decoder, self).__init__()\n",
    "    # Define Fully-Connected FeedForward Connections\n",
    "    self.hidden = nn.Linear(in_features=latent_dim, out_features=d_hidden)\n",
    "    # Second set of FC connections. Here we only want one output\n",
    "    self.output_layer = nn.Linear(in_features=d_hidden, out_features=28*28)\n",
    "\n",
    "  def forward(self, z):\n",
    "    \"\"\"Defines how the network transforms the latent input z into a flatten image.\"\"\"\n",
    "    # Notice that we use a sigmoid function at the end to restrict output values between \n",
    "    # 0 and 1 so they can be interpreted as probabilities (?)\n",
    "    z = F.relu(self.hidden(z))\n",
    "    return torch.sigmoid(self.output_layer(z))\n",
    "\n",
    "class VAE_PF(nn.Module):\n",
    "  def __init__(self, k):\n",
    "    \"\"\"Puts together Encoder & Decoder with the reparametrization trick.\"\"\"\n",
    "    super(VAE, self).__init__()\n",
    "    self.encoder = EncoderPF()\n",
    "    self.decoder = DecoderPF()\n",
    "    self.k = k\n",
    "\n",
    "  def sample_latent(self, mu, logvar):\n",
    "    if self.training:\n",
    "      # Get standard normal in the shape of mu\n",
    "      eps = torch.randn_like(mu)\n",
    "      # Use logarithmic properties to transform logvar to std. Then multiply\n",
    "      # and sum by latent mu\n",
    "      return eps.mul(torch.exp(0.5*logvar)).add_(mu)\n",
    "    else:   # This is used when testing \n",
    "      return mu    \n",
    "\n",
    "  @staticmethod\n",
    "  def get_uhat(w, u):\n",
    "    \"\"\"Finds u hat\"\"\"\n",
    "    def m(x):\n",
    "        return x.exp().add(1).log().add(-1)\n",
    "    dot = torch.dot(w.flatten(), u.flatten())\n",
    "    return u + ((m(dot) - dot) * w) / (torch.norm(w)**2)\n",
    "\n",
    "  def flow(self, z, u, w, b):\n",
    "    \"\"\"NF transformation\"\"\"\n",
    "    for k in range(self.k):\n",
    "        z = z + u*torch.dot(w.flatten(), z.flatten()).add(b).tanh()\n",
    "    return z\n",
    "    \n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"Transforms image into latent and then back to its reconstruction.\"\"\"\n",
    "    # Feed image to encoder. Obtain mean and logvar for the latent space\n",
    "    latent_mu, latent_logvar, w, u, b = self.encoder(x.view(-1, 28*28))\n",
    "    # Sample from the latent space with the give mean and variance using the \n",
    "    # reparametrization trick\n",
    "    z = self.sample_latent(latent_mu, latent_logvar)\n",
    "    # First, get u_hat (look at appendix)\n",
    "    uhat = self.get_uhat(w, u)\n",
    "    # Feed sample through transformation k times\n",
    "    zK = self.flow(z, uhat, w, b)\n",
    "    # Decode the latent representation\n",
    "    return self.decoder(zK), latent_mu, latent_logvar, w, uhat, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "def vaeNF_loss(image, reconstruction, mu, logvar, w, uhat, b):\n",
    "  \"\"\"Loss for the Variational AutoEncoder.\"\"\"\n",
    "  # Compute the binary_crossentropy.\n",
    "  # Notice that we reshape them because in practice we don't receive just 1 image and 1 reconstruction, but we receive a whole batch!\n",
    "  reconstruction_loss = F.binary_cross_entropy(input=reconstruction.view(-1, 28*28), target=image.view(-1, 28*28), reduction='sum')\n",
    "  # Compute KL divergence using formula (closed-form)\n",
    "  kl = 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "  return reconstruction_loss - kl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
