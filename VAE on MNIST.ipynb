{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT\n",
    "import numpy as np                             # for fast array manipulation\n",
    "import torch                                   # Pytorch\n",
    "import torchvision                             # contains image datasets and many functions to manipulate images\n",
    "import torchvision.transforms as transforms    # to normalize, scale etc the dataset\n",
    "from torch.utils.data import DataLoader        # to load data into batches (for SGD)\n",
    "from torchvision.utils import make_grid        # Plotting. Makes a grid of tensors\n",
    "from torchvision.datasets import MNIST         # the classic handwritten digits dataset\n",
    "import matplotlib.pyplot as plt                # to plot our images\n",
    "import torch.nn as nn                          # Class that implements a model (such as a Neural Network)\n",
    "import torch.nn.functional as F                # contains activation functions, sampling layers and more \"functional\" stuff\n",
    "import torch.optim as optim                    # For optimization routines such as SGD, ADAM, ADAGRAD, etc\n",
    "from torch.distributions import MultivariateNormal\n",
    "from math import log, pi\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Settings\n",
    "batch_size = 100      # How many images to use for a SGD update\n",
    "L = 1                 # Samples per data point. See section \"Likelihood Lower Bound\".\n",
    "e_hidden = 500        # Number of hidden units in the encoder. Chosen based on AEVB paper page 7, section \"Marginal Likelihood\"\n",
    "d_hidden = 500        # Number of hidden units in the decoder. Chosen based on AEVB paper page 7, section \"Marginal Likelihood\"\n",
    "latent_dim = 2        # Chosen based on AEVB paper, page 7, section \"Marginal Likelihood\"\n",
    "learning_rate = 0.001 # For SGD\n",
    "weight_decay = 1e-5   # For SGD\n",
    "epochs = 10        # Number of sweeps through the whole dataset, also called epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "t = transforms.Compose([\n",
    "                        transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Use transformation for both training and test set\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=t)\n",
    "testset  = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=t)\n",
    "\n",
    "# Load train and test set\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader  = DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder, Decoder and VAE\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "    Encoder Network. Must inherit from `nn.Module` provided by Pytorch. We only need to define 2 things:\n",
    "    \n",
    "    1) The components of the network (layers, activation functions, etc). This is done in __init__().\n",
    "    2) How the network uses such components to transform the network input into an output. This is done in a method called `forward()`.\n",
    "    \"\"\"\n",
    "    super(Encoder, self).__init__()\n",
    "    # Define Fully-Connected FeedForward Connections\n",
    "    self.hidden = nn.Linear(in_features=28*28, out_features=e_hidden)\n",
    "    # We need two separate layers. One is used for mu one is used for logvar.\n",
    "    self.mu_layer     = nn.Linear(in_features=e_hidden, out_features=latent_dim)\n",
    "    self.logvar_layer = nn.Linear(in_features=e_hidden, out_features=latent_dim)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    \"\"\"Defines how the network transforms the input x into an encoded representation.\"\"\"\n",
    "    # Pass input through the first set of connections\n",
    "    x = F.relu(self.hidden(x))\n",
    "    # Now pass it to one set of connections to get mu, and to another set of connections \n",
    "    # to get logvar\n",
    "    return self.mu_layer(x), self.logvar_layer(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "    Decoder Network. Works similarly to the encoder, except it takes an input from the latent space\n",
    "    and then outputs an image.\n",
    "    \"\"\"\n",
    "    super(Decoder, self).__init__()\n",
    "    # Define Fully-Connected FeedForward Connections\n",
    "    self.hidden = nn.Linear(in_features=latent_dim, out_features=d_hidden)\n",
    "    # Second set of FC connections. Here we only want one output\n",
    "    self.output_layer = nn.Linear(in_features=d_hidden, out_features=28*28)\n",
    "\n",
    "  def forward(self, z):\n",
    "    \"\"\"Defines how the network transforms the latent input z into a flatten image.\"\"\"\n",
    "    # Notice that we use a sigmoid function at the end to restrict output values between \n",
    "    # 0 and 1 so they can be interpreted as probabilities (?)\n",
    "    z = F.relu(self.hidden(z))\n",
    "    return torch.sigmoid(self.output_layer(z))\n",
    "\n",
    "class VAE(nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"Puts together Encoder & Decoder with the reparametrization trick.\"\"\"\n",
    "    super(VAE, self).__init__()\n",
    "    self.encoder = Encoder()\n",
    "    self.decoder = Decoder()\n",
    "\n",
    "  def sample_latent(self, mu, logvar):\n",
    "    if self.training:\n",
    "      # Get standard normal in the shape of mu\n",
    "      eps = torch.randn_like(mu)\n",
    "      # Use logarithmic properties to transform logvar to std. Then multiply\n",
    "      # and sum by latent mu\n",
    "      return eps.mul(torch.exp(0.5*logvar)).add_(mu)\n",
    "    else:   # This is used when testing \n",
    "      return mu    \n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"Transforms image into latent and then back to its reconstruction.\"\"\"\n",
    "    # Feed image to encoder. Obtain mean and logvar for the latent space\n",
    "    latent_mu, latent_logvar = self.encoder(x.view(-1, 28*28))\n",
    "    # Sample from the latent space with the give mean and variance using the \n",
    "    # reparametrization trick\n",
    "    z = self.sample_latent(latent_mu, latent_logvar)\n",
    "    # Decode the latent representation\n",
    "    return self.decoder(z), latent_mu, latent_logvar   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "def vae_loss(image, reconstruction, mu, logvar):\n",
    "  \"\"\"Loss for the Variational AutoEncoder.\"\"\"\n",
    "  # Compute the binary_crossentropy.\n",
    "  # Notice that we reshape them because in practice we don't receive just 1 image and 1 reconstruction, but we receive a whole batch!\n",
    "  reconstruction_loss = F.binary_cross_entropy(input=reconstruction.view(-1, 28*28), target=image.view(-1, 28*28), reduction='sum')\n",
    "  # Compute KL divergence using formula (closed-form)\n",
    "  kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "  return reconstruction_loss + kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 10] average reconstruction error: 18466.625936\n",
      "Epoch [2 / 10] average reconstruction error: 16632.391872\n",
      "Epoch [3 / 10] average reconstruction error: 16274.567900\n",
      "Epoch [4 / 10] average reconstruction error: 16048.492209\n",
      "Epoch [5 / 10] average reconstruction error: 15883.186273\n",
      "Epoch [6 / 10] average reconstruction error: 15753.615415\n",
      "Epoch [7 / 10] average reconstruction error: 15639.520076\n",
      "Epoch [8 / 10] average reconstruction error: 15546.366307\n",
      "Epoch [9 / 10] average reconstruction error: 15467.203861\n",
      "Epoch [10 / 10] average reconstruction error: 15397.673185\n"
     ]
    }
   ],
   "source": [
    "# Instantiate VAE\n",
    "vae = VAE()\n",
    "\n",
    "# Pass VAE to the device (GPU or CPU)\n",
    "vae = vae.to(device)\n",
    "\n",
    "# Use Stochastic Gradient Descent as optimizer\n",
    "optimizer = optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Set VAE to training mode\n",
    "vae.train()\n",
    "\n",
    "# Store all losses here\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  # Put a zero into losses. This is where we will cumulatively sum all the losses\n",
    "  # from each batch. After all batches are done, we will divide by the number of batches\n",
    "  # to obtain the average loss per batch\n",
    "  losses.append(0)\n",
    "\n",
    "  # To compute the number of batches (since it varies depending on dataset size)\n",
    "  # update a counter variable\n",
    "  number_of_batches = 0\n",
    "\n",
    "  # Grab the batch, we are only interested in images not on their labels\n",
    "  for images, _ in trainloader:\n",
    "    # Store image batch to device\n",
    "    images = images.to(device)\n",
    "\n",
    "    # Set previous gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Feed images through the VAE to obtain their reconstruction\n",
    "    reconstructions, latent_mu, latent_logvar = vae(images)\n",
    "\n",
    "    # Compare reconstruction and images via the loss function\n",
    "    loss = vae_loss(images, reconstructions, latent_mu, latent_logvar)\n",
    "\n",
    "    # Backpropagate the loss. Before doing that, make sure all previously stored gradients are zero (e.g. from previous iterations)\n",
    "    loss.backward()\n",
    "\n",
    "    # Use the accumulated gradients to do a step in the right direction\n",
    "    optimizer.step()\n",
    "\n",
    "    # Add loss to the cumulative sum\n",
    "    losses[-1] += loss.item()        #.item() grabs the value in a tensor with only 1 value\n",
    "    number_of_batches += 1\n",
    "  \n",
    "  # At the end of all batches divide the total loss for this epoch by the number of \n",
    "  # batches to obtain an average loss per batch\n",
    "  losses[-1] /= number_of_batches\n",
    "  print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, epochs, losses[-1]))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE with Normalizing Flows (Planar Flows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory of Vanilla VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a vanilla VAE we feed $x$ into an encoder neural network and obtain $(\\mu, \\log \\sigma)$. These are the parameters of our approximate distribution \n",
    "\n",
    "$$\n",
    "q_{\\phi}(z \\mid x) = \\mathcal{N}(z \\mid \\mu_{\\phi}(x), \\sigma^2_{\\phi}(x) I)\n",
    "$$\n",
    "\n",
    "We then get a sample $z \\sim q_{\\phi}(z \\mid x)$ by using the reparametrization trick $z = \\mu + \\sigma \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, 1)$. We then use the following objective function\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\phi, \\theta}(x) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] - \\text{KL}(q_{\\phi}(z \\mid x) \\parallel p(z))\n",
    "$$\n",
    "\n",
    "where we compute the KL divergence in closed form using \n",
    "\n",
    "$$\n",
    "\\text{KL}(q_{\\phi}(z \\mid x) \\parallel p(z)) = -\\frac{1}{2}\\sum_{j=1}^{\\text{dim}(z)} \\left(1 + \\log\\sigma_j^2 - \\mu_j^2 - \\sigma_j^2\\right)\n",
    "$$\n",
    "\n",
    "and we can compute the reconstruction error in two simple cases: Bernoulli and Normal. In the Bernoulli case (e.g. when working with images) we have\n",
    "\n",
    "$$\n",
    "p_{\\theta}(x \\mid z) = \\prod_{i=1}^{\\text{dim}(x)} p_i(z)^{x_i}(1 - p_i(z))^{1 - x_i}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "p = (p_1(z), \\ldots, p_{\\text{dim}(x)}(z))^\\top \n",
    "$$\n",
    "is the output of the decoder network: $z \\longrightarrow $ Decoder $ \\longrightarrow p \\in [0, 1]^{\\text{dim}(x)}$. This means that we can write the reconstruction error as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)]\n",
    "    &=  \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[\\log \\prod_{i=1}^{\\text{dim}(x)} p_i(z)^{x_i}(1 - p_i(z))^{1-  x_i}\\right] \\\\\n",
    "    &= \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[\\sum_{i=1}^{\\text{dim}(x)} x_i \\log p_i(z) + (1 - x_i) \\log(1 - p_i(z))\\right] \\\\\n",
    "    &\\approx \\sum_{j=1}^{n_{z}}\\sum_{i=1}^{\\text{dim}(x)} x_i \\log p_i(z) + (1 - x_i) \\log(1 - p_i(z)) \\qquad z^{(j)} \\sim q_{\\phi}(z \\mid x)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $n_z$ is the number of samples that we sample from $q_{\\phi}(z \\mid x)$. Usually, we simply set $n_z = 1$, that is we only sample one latent variable for each datapoint. This leads to the following objective function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\phi, \\theta}(x) = \\sum_{i=1}^{\\text{dim}(x)} x_i \\log p_i(z^{(j)}) + (1 - x_i) \\log(1 - p_i(z^{(j)})) +\\frac{1}{2}\\sum_{j=1}^{\\text{dim}(z)} \\left(1 + \\log\\sigma_j^2 - \\mu_j^2 - \\sigma_j^2\\right)\n",
    "$$\n",
    "\n",
    "which was coded as follow:\n",
    "\n",
    "```python\n",
    "def vae_loss(image, reconstruction, mu, logvar):\n",
    "  \"\"\"Loss for the Variational AutoEncoder.\"\"\"\n",
    "  # Compute the binary_crossentropy.\n",
    "  recon_loss = F.binary_cross_entropy(\n",
    "      input=reconstruction.view(-1, 28*28),    # input is p(z) (the mean reconstruction)\n",
    "      target=image.view(-1, 28*28),            # target is x   (the true image)\n",
    "      reduction='sum'                          \n",
    "  )\n",
    "  # Compute KL divergence using formula (closed-form)\n",
    "  kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "  return reconstruction_loss + kl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory of Normalizing Flows VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we not only want our Encoder to output $(\\mu, \\log \\sigma)$ to shift and scale our standard normal $\\epsilon \\sim \\mathcal{N}(0, 1)$. We also want to feed it through a series of transformations depending on some parameters $\\lambda$. In particular, we would like our Encoder to work as follows:\n",
    "\n",
    "$$\n",
    "x \\longrightarrow \\text{Encoder} \\longrightarrow (\\mu, \\log\\sigma, \\lambda)\n",
    "$$\n",
    "\n",
    "then we would use $(\\mu, \\log\\sigma)$ to compute $z_0$ using the reparametrization trick\n",
    "\n",
    "$$\n",
    "z_0 = \\mu + \\sigma \\epsilon \\qquad \\epsilon \\sim \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "and finally, we would feed $z_0$ into a series of transformation (with parameters $\\lambda$) to reach the final $z_K$:\n",
    "\n",
    "$$\n",
    "z_K = f_K \\circ \\ldots \\circ f_2 \\circ f_1(z_0)\n",
    "$$\n",
    "\n",
    "This means that our approximating distribution would not be \n",
    "\n",
    "$$\n",
    "q_{\\phi}(z \\mid x) = \\mathcal{N}(z \\mid \\mu_{\\phi}(x), \\sigma^2_{\\phi}(x) I)\n",
    "$$\n",
    "\n",
    "anymore but rather, we would have $q_0(z_0)=\\mathcal{N}(z_0 \\mid \\mu_{\\phi}(x), \\sigma_{\\phi}^2(x) I)$ and \n",
    "(using the change of variables formula)\n",
    "\n",
    "$$\n",
    "\\ln q_{\\phi}(z \\mid x) = \\ln q_K(z_K) = \\ln q_0(z_0) - \\sum_{k=1}^K \\ln \\left|\\text{det}\\frac{\\partial f_k}{\\partial z_{k-1}}\\right|.\n",
    "$$\n",
    "Thanks to the law of the uncounscious statistician we now know that \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{\\phi, \\theta}(x) \n",
    "&= \\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] - \\text{KL}(q_{\\phi}(z \\mid x) \\parallel p(z)) \\\\\n",
    "&= \\mathbb{E}_{q_K(z_K)}[\\log p_{\\theta}(x \\mid z_K)] - \\mathbb{E}_{q_K(z_K)}[\\log q_K(z_K) - \\log p(z_K)] \\\\\n",
    "&= \\mathbb{E}_{q_0(z_0)}[\\log p_{\\theta}(x \\mid f_K \\circ \\ldots \\circ f_2 \\circ f_1(z_0))] - \\mathbb{E}_{q_0(z_0)}[\\log q_K(f_K \\circ \\ldots \\circ f_2 \\circ f_1(z_0)) - \\log p(f_K \\circ \\ldots \\circ f_2 \\circ f_1(z_0))]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "as usual, we can approximate this with Monte Carlo by drawing samples $z_0 \\sim q_0(z_0) = N(\\mu, \\sigma^2 I)$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\phi, \\theta}(x) \\approx \\sum_{j=1}^{n_z}\\log p_{\\theta}(x \\mid f_K \\circ \\ldots \\circ f_2 \\circ f_1(z_0^j)) - \\left[\\sum_{j=1}^{n_z}\\log q_K(f_K \\circ \\ldots \\circ f_2 \\circ f_1(z_0^j)) - \\log p(f_K \\circ \\ldots \\circ f_2 \\circ f_1(z_0^j))\\right]\n",
    "$$\n",
    "\n",
    "Again, in practice we use only one sample\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{\\phi, \\theta}(x) \n",
    "&\\approx \\left[\\sum_{i=1}^{\\text{dim}(x)} x_i \\log p_i(z_K) + (1 - x_i) \\log(1 - p_i(z_K))\\right] - \\log q_K(z_K) + \\log p(z_K) \\\\\n",
    "&= -\\text{BCE}(X, z_K) - (\\log q_0(z_0) + \\text{LADJ}) + \\log p(z_K) \\\\\n",
    "&= -\\text{BCE}(X, z_K) - \\log q_0(z_0) - \\text{LADJ} + \\log p(z_K)\n",
    "\\end{align}\n",
    "$$\n",
    "Pycharm however does minimization only, so our objective becomes\n",
    "$$\n",
    "\\text{BCE}(X, z_K) + \\log q_0(z_0) + \\text{LADJ} -\\log p(z_K)\n",
    "$$\n",
    "Using temperature we then have\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{objective} \n",
    "    &= -\\sum_{i=1}^{\\text{dim}(x)} x_i \\log p_i(z_K) + (1 - x_i) \\log(1 - p_i(z_K)) \\\\\n",
    "    &\\quad -\\frac{d}{2}\\log(2\\pi) -\\frac{1}{2}\\log \\text{det}(\\text{Diag}(\\sigma^2))- \\frac{1}{2}(x-\\mu)^\\top \\text{Diag}\\left(\\frac{1}{\\sigma^2}\\right)(x - \\mu) \\\\\n",
    "    &\\quad -\\sum_{k=1}^K \\log |1 + u_k^\\top (1 - \\tanh^2(w_k^\\top z_{k-1} + b_k))w_k| \\\\\n",
    "    &\\quad -\\frac{d}{2}\\log(2\\pi) -\\frac{1}{2}x^\\top x\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In our case we use the transformation\n",
    "\n",
    "$$\n",
    "f(z) = u h(w^\\top z + b) \\qquad u, w \\in \\mathbb{R}^{\\text{dim}(z)\\times 1} \\qquad b\\in\\mathbb{R} \\qquad h(\\cdot)  =\\tanh(\\cdot)\n",
    "$$\n",
    "\n",
    "It's Log-Absolute-Determinant-Jacobian (LADJ) is given by\n",
    "\n",
    "$$\n",
    "\\log \\left|\\text{det}\\frac{\\partial f}{\\partial z}\\right| = \\log |1 + u^\\top h'(w^\\top z + b)w|\n",
    "$$\n",
    "\n",
    "where $h'$ is the derivative of $h$ and when $h = \\tanh$ we have\n",
    "$$\n",
    "h'(\\cdot) = 1 - \\tanh^2(\\cdot)\n",
    "$$\n",
    "\n",
    "If we apply this transformation $K$ times we get \n",
    "\n",
    "$$\n",
    "\\text{LADJ} = -\\sum_{k=1}^K \\log |1 + u_k^\\top (1 - \\tanh^2(w_k^\\top z_{k-1} + b_k))w_k|\n",
    "$$\n",
    "\n",
    "**NOTE**: we have a different set of $\\lambda_k = \\{w_k, u_k, b_k\\}$ for every transformation. \n",
    "\n",
    "Finally, not all transformations like this are invertible. One way to obtain an invertible transformation is by modifying $u$ after it has come out of the encoder, and replace it with\n",
    "\n",
    "$$\n",
    "\\widehat{u} = u + \\left[-1 + \\log(1 + e^{w^\\top u}) - w^\\top u\\right] \\frac{w}{\\parallel w \\parallel^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla VAE - Minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_vanilla(nn.Module):\n",
    "    def __init__(self, e_hidden, d_hidden, latent_dim):\n",
    "        \"\"\"Variational Auto-Encoder Class\"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoding Layers\n",
    "        self.e_input2hidden = nn.Linear(in_features=784, out_features=e_hidden)\n",
    "        self.e_hidden2mean = nn.Linear(in_features=e_hidden, out_features=latent_dim)\n",
    "        self.e_hidden2logvar = nn.Linear(in_features=e_hidden, out_features=latent_dim)\n",
    "        \n",
    "        # Decoding Layers\n",
    "        self.d_latent2hidden = nn.Linear(in_features=latent_dim, out_features=d_hidden)\n",
    "        self.d_hidden2image = nn.Linear(in_features=d_hidden, out_features=784)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Shape Flatten image to [batch_size, input_features]\n",
    "        x = x.view(-1, 784)\n",
    "        \n",
    "        # Feed x into Encoder to obtain mean and logvar\n",
    "        x = F.relu(self.e_input2hidden(x))\n",
    "        mu, logvar = self.e_hidden2mean(x), self.e_hidden2logvar(x)\n",
    "        \n",
    "        # Sample z from latent space using mu and logvar\n",
    "        if self.training:\n",
    "            z = torch.randn_like(mu).mul(torch.exp(0.5*logvar)).add_(mu)\n",
    "        else:\n",
    "            z = mu\n",
    "        \n",
    "        # Feed z into Decoder to obtain reconstructed image. Use Sigmoid as output activation (=probabilities)\n",
    "        x_recon = torch.sigmoid(self.d_hidden2image(torch.relu(self.d_latent2hidden(z))))\n",
    "        \n",
    "        return x_recon, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Amortized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that our input $x$ to the encoder will actually be of size $(\\text{batch_size}, 784)$. This means that the outputs of the Encoder (in this amortized version) will have dimensions:\n",
    "\n",
    "- $\\mu$ has dimension $(\\text{batch_size}, 2)$\n",
    "- $\\log \\sigma^2$ has dimensions $(\\text{batch_size}, 2)$\n",
    "- $w$, and $u$ have dimensions $(\\text{batch_size}, 2)$\n",
    "- $b$ has dimensions $(\\text{batch_size}, 1)$\n",
    "\n",
    "This means that when computing $w^\\top z_k$ we need to be careful. The only way to make this work with batch matrices is by computing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_PF_amortized(nn.Module):\n",
    "    def __init__(self, e_hidden, d_hidden, latent_dim, K):\n",
    "        \"\"\"VAE with Planar Flows - Amortized\"\"\"\n",
    "        super(VAE_PF_amortized, self).__init__()\n",
    "        # Encoding Layers\n",
    "        self.e_input2hidden = nn.Linear(in_features=784, out_features=e_hidden)\n",
    "        self.e_hidden2mean = nn.Linear(in_features=e_hidden, out_features=latent_dim)\n",
    "        self.e_hidden2logvar = nn.Linear(in_features=e_hidden, out_features=latent_dim)\n",
    "        self.e_hidden2w = nn.Linear(in_features=e_hidden, out_features=latent_dim*K) # AMORTIZED!\n",
    "        self.e_hidden2u = nn.Linear(in_features=e_hidden, out_features=latent_dim*K) # AMORTIZED!\n",
    "        self.e_hidden2b = nn.Linear(in_features=e_hidden, out_features=K)          # AMORTIZED!\n",
    "        \n",
    "        # Decoding Layers\n",
    "        self.d_latent2hidden = nn.Linear(in_features=latent_dim, out_features=d_hidden)\n",
    "        self.d_hidden2image = nn.Linear(in_features=d_hidden, out_features=784)\n",
    "        \n",
    "        # Store setting\n",
    "        self.K = K\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.e_input2hidden(x))\n",
    "        # mu, sigma for latent space\n",
    "        mu, logvar = self.e_hidden2mean(x), self.e_hidden2logvar(x)\n",
    "        # parameters for normalizing flow\n",
    "        w, u, b = self.e_hidden2w(x), self.e_hidden2u(x), self.e_hidden2b(x)\n",
    "        # reshape\n",
    "        batch_size = x.size(0)\n",
    "        w = w.view(batch_size, self.K, 1, latent_dim)\n",
    "        u = u.view(batch_size, self.K, latent_dim, 1)  # this is inverted so we can do dot product later\n",
    "        b = b.view(batch_size, self.K, 1, 1)\n",
    "        return mu, logvar, w, u, b\n",
    "    \n",
    "    def flow(self, z0, w, u, b):\n",
    "        z_k = z0\n",
    "        ladj_sum = torch.zeros((100, 1))\n",
    "        \n",
    "        # Feed z0 into flow. \n",
    "        for k in range(self.K):\n",
    "            # Grab parameters for this flow\n",
    "            w_k = w[:, k, :, :]   # (batch_size, 1, 2)\n",
    "            u_k = u[:, k, :, :]   # (batch_size, 2, 1)\n",
    "            b_k = b[:, k, :, :]   # (batch_size, 1, 1)\n",
    "            # z0: (batch_size, 2, 1)            \n",
    "            \n",
    "            # Obtain u_hats so that transformations are invertible (see paper appendix).\n",
    "            # This part is taken from https://github.com/federicobergamin/Variational-Inference-with-Normalizing-Flows/blob/master/flows_with_amortized_weights.py\n",
    "            uw = torch.bmm(w_k, u_k)  # size (batch_size, 1, 1) basically a dot product for each batch\n",
    "            m_uw = -1 + F.softplus(uw) # size (batch_size, 1, 1)\n",
    "            # w has shape (batch_size, 1, 2). Transpose last two dimensions so it has size (batch_size, 2, 1) same as u\n",
    "            uhat_k = u_k + ((m_uw - uw)* w_k.transpose(2, 1) / (torch.norm(w_k, dim=2, keepdim=True)**2))\n",
    "            #print(\"### DIM1: \", (m_uw - uw).size())\n",
    "            #print(\"### DIM2: \", w_k.transpose(2, 1).size())\n",
    "            #print(\"### DIM3: \", ((m_uw - uw)* w_k.transpose(2, 1)).size())\n",
    "            #print(\"### DIM4: \", (torch.norm(w_k, dim=2, keepdim=True)**2).size())\n",
    "            # Now we use uhat instead of u. Since w_k is (batch_size, 2, 1) and zk has dimension \n",
    "            # (batch_size, 2, 1) we can do batch matrix multiplication with torch.bmm(). \n",
    "            # uhat has dimensions (batch_size, 2, 1) and similarly does z_k. The tanh term has dimensions\n",
    "            # (batch_size, 1, 1). The resulting z_{k+1} will thus have dimensions (batch_size, 2, 1).\n",
    "            # For this reason, we squeeze the last dimension out to get (batch_size, 2).\n",
    "            wz_plus_b = torch.bmm(w_k, z_k)+ b_k\n",
    "            #print(\"thing to be added: \", (uhat_k * torch.tanh(wz_plus_b))[:2, 0, 0])\n",
    "            #print(\"uhat_k: \", uhat_k[:2, 0,0])\n",
    "            #print(\"factor: \", torch.tanh(wz_plus_b)[:2, 0, 0])\n",
    "            z_k_plus_1 = (z_k + uhat_k * torch.tanh(wz_plus_b)).squeeze(2)\n",
    "            \n",
    "            h_prime = (1 - torch.tanh(wz_plus_b)**2)\n",
    "            #print(\"h_prime: \", h_prime.sum())\n",
    "            #print(\"bmm: \", torch.bmm(h_prime*w_k, uhat_k).sum())\n",
    "            #print(\"abs: \", (1 + torch.bmm(h_prime*w_k, uhat_k)).abs().sum())\n",
    "            #print(\"add: \", (1 + torch.bmm(h_prime*w_k, uhat_k)).abs().add(1e-8).sum())\n",
    "            #print(\"log: \", (1 + torch.bmm(h_prime*w_k, uhat_k)).abs().add(1e-8).log().sum())\n",
    "            #print(\"log size: \", (1 + torch.bmm(h_prime*w_k, uhat_k)).abs().add(1e-8).log().size())\n",
    "            #print(\"LOL : \", -(1 + torch.bmm(h_prime*w_k, uhat_k)).abs().add(1e-8).log().squeeze(2).sum())\n",
    "            # Compute LADJ. Would be of size (batch_size, 1, 1) so we squeeze out the last dimension.\n",
    "            ladj = -(1 + torch.bmm(h_prime*w_k, uhat_k)).abs().add(1e-8).log().squeeze(2)\n",
    "            #ladj = -torch.log(torch.abs(1 + ))\n",
    "            #wz_plus_b\n",
    "            ladj_sum += ladj\n",
    "            #print(\"ladj: \", ladj.size())\n",
    "            # Set current z_k to the new one\n",
    "            zk = z_k_plus_1\n",
    "            #print(\"-\"*80)\n",
    "        \n",
    "        z0, z_k = z0.squeeze(2), z_k.squeeze(2)\n",
    "        return z0, zk, ladj_sum\n",
    "    \n",
    "    def decode(self, z0):\n",
    "        z0, z_k = self.flow(z0)\n",
    "        return torch.sigmoid(self.d_hidden2image(torch.relu(self.d_latent2hidden(z_k))))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Shape Flatten image to [batch_size, input_features]\n",
    "        x = x.view(-1, 784)\n",
    "        \n",
    "        mu, logvar, w, u, b = self.encode(x)\n",
    "        # Feed x into Encoder to obtain mean and logvar \n",
    "        #x = F.relu(self.e_input2hidden(x))\n",
    "        #mu, logvar = self.e_hidden2mean(x), self.e_hidden2logvar(x)  # (batch_size, latent_dim)\n",
    "        # Get also amortized parameters for the flow\n",
    "        # w has dimensions (batch_size, latent_dim*K)\n",
    "        # u has dimensions (batch_size, latent_dim*K)\n",
    "        # b has dimensions (batch_size, K)\n",
    "        #w, u, b = self.e_hidden2w(x), self.e_hidden2u(x), self.e_hidden2b(x)\n",
    "        \n",
    "        # Reshape so we can do computations \"across batch size\"\n",
    "        #batch_size = x.size(0)\n",
    "        #w = w.view(batch_size, self.K, 1, latent_dim)\n",
    "        #u = u.view(batch_size, self.K, latent_dim, 1)  # this is inverted so we can do dot product later\n",
    "        #b = b.view(batch_size, self.K, 1, 1)\n",
    "        \n",
    "        # Sample z0 from latent space using mu and logvar\n",
    "        # z0 has size (batch_size, latent_dim) but we add an extra dimension to be able to do batch \n",
    "        # computations. z0 becomes (batch_size, latent_dim, 1)\n",
    "        if self.training:\n",
    "            z0 = torch.randn_like(mu).mul(torch.exp(0.5*logvar)).add_(mu).unsqueeze(2)\n",
    "        else:\n",
    "            z0 = mu.unsqueeze(2)\n",
    "                  \n",
    "        #z0, z_k = self.flow(z0)\n",
    "        #print(\"After NF, after squeezing.\")\n",
    "        #print(\"z0: \", z0.size())\n",
    "        #print(\"z_k: \", z_k.size())\n",
    "        \n",
    "        # Now feed z_K into the decoder to obtain p vector (mean reconstruction)\n",
    "        #recon = torch.sigmoid(self.d_hidden2image(torch.relu(self.d_latent2hidden(z_k))))\n",
    "        #print(\"recon: \", recon.size())\n",
    "        z0, z_k, ladj_sum = self.flow(z0, w, u, b)\n",
    "        recon = torch.sigmoid(self.d_hidden2image(torch.relu(self.d_latent2hidden(z_k))))\n",
    "        \n",
    "        \n",
    "        # Compute KL divergence. First of all we need to compute logq0\n",
    "        # Should be (batch_size, 1)\n",
    "        #print(\"z0-mu:\", (z0-mu).size())\n",
    "        #print(\"2*torch.exp(logvar):\", (2*torch.exp(logvar)).size())\n",
    "        #print(\"(z0 - mu)/(2*torch.exp(logvar): \", ((z0 - mu)/(2*torch.exp(logvar))).size())\n",
    "        #print(\"((z0 - mu)/(2*torch.exp(logvar))).unsqueeze(1): \", (((z0 - mu)/(2*torch.exp(logvar))).unsqueeze(1)).size())\n",
    "        #print(\"(z0-mu).unsqueeze(2): \", ((z0-mu).unsqueeze(2)).size())\n",
    "        #print(\"bmm: \", (torch.bmm(((z0 - mu)/(2*torch.exp(logvar))).unsqueeze(1), (z0-mu).unsqueeze(2))).size())\n",
    "        #print(\"bmm squeeze: \", (torch.bmm(((z0 - mu)/(2*torch.exp(logvar))).unsqueeze(1), (z0-mu).unsqueeze(2)).squeeze(2)).size())\n",
    "        #print(\"logvar sum: \", logvar.sum(dim=1, keepdim=True).size())\n",
    "        log_q0 = -log(2*pi) -logvar.add(1e-8).sum(dim=1, keepdim=True) - torch.bmm(((z0 - mu).add(1e-8)/(2*torch.exp(logvar))).unsqueeze(1), (z0-mu).unsqueeze(2)).squeeze(2)\n",
    "        #print(\"log_q0: \", log_q0.size())\n",
    "        #print(\"ladj_sum: \", ladj_sum.size())\n",
    "        # Now compute log_qk by subtracting ladj_sum to log_q0\n",
    "        ############### IMPORTANT, I'VE COMMENTED THIS ONE OUT TO TEST IT\n",
    "        #log_qK = log_q0 + ladj_sum\n",
    "        #####################################################################\n",
    "        #print(\"log_qK: \", log_qK.size())\n",
    "        # Finally compute log p(z_K) for the prior. Recall zk is (batch_size, 2) so essentially we compute the\n",
    "        # batch multiplication between z_k with dimension (batch_size, 1, 2) and (batch_size, 2, 1) so that\n",
    "        # the result is (batch_size, 1, 1). Then we squeeze out the last dimension to obtain (batch_size, 1)\n",
    "        log_pK = -log(2*pi) -0.5*torch.bmm(z_k.unsqueeze(1), z_k.unsqueeze(2)).squeeze(2).add(1e-8)\n",
    "        #print(\"log_pK: \", log_pK.size())\n",
    "        #print(\"kl: \", (log_qK - log_pK).size())\n",
    "        # return reconstruction and terms for kl divergence\n",
    "        return recon, log_q0, ladj_sum, log_pK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_pf_loss(x, recon, log_qk, log_pk, beta):\n",
    "    \"\"\"Loss for VAE with PF\"\"\"\n",
    "    #print(\"-\"*80)\n",
    "    #print(\"Inside VAE loss\")\n",
    "    #print(\"x: \", x.size())\n",
    "    #print(\"recon: \", recon.size())\n",
    "    #print(\"kl: \", kl.size())\n",
    "    #print(\"VAE Loss. x range ({}, {})\".format(x.min(), x.max()))\n",
    "    #print(\"VAE Loss. recon range ({}, {})\".format(recon.min(), recon.max()))\n",
    "    #print(\"VAE Loss. recon nans: \",torch.isnan(recon.view(-1)).sum())\n",
    "    #print(\"recon loss: \", recon_loss.size())\n",
    "    #print(\"recon_loss: \", recon_loss)\n",
    "    #BCE = F.binary_cross_entropy(recon.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
    "    #KL  = (log_qk - beta*log_pk).sum()\n",
    "    return 0 #beta*BCE + KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 10] average reconstruction error: 18068.218122\n",
      "Epoch [2 / 10] average reconstruction error: 15782.609670\n",
      "Epoch [3 / 10] average reconstruction error: 15235.382275\n",
      "Epoch [4 / 10] average reconstruction error: 14921.211912\n",
      "Epoch [5 / 10] average reconstruction error: 14754.922241\n",
      "Epoch [6 / 10] average reconstruction error: 14591.640799\n",
      "Epoch [7 / 10] average reconstruction error: 14394.930627\n",
      "Epoch [8 / 10] average reconstruction error: 14287.859533\n",
      "Epoch [9 / 10] average reconstruction error: 14204.706882\n",
      "Epoch [10 / 10] average reconstruction error: 14102.411489\n"
     ]
    }
   ],
   "source": [
    "# Instantiate VAE\n",
    "vae_pf = VAE_PF_amortized(e_hidden, d_hidden, latent_dim, K=3)\n",
    "\n",
    "# Pass VAE to the device (GPU or CPU)\n",
    "vae_pf = vae_pf.to(device)\n",
    "\n",
    "# Use Stochastic Gradient Descent as optimizer\n",
    "optimizer_pf = optim.Adam(params=vae_pf.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Set VAE to training mode\n",
    "vae_pf.train()\n",
    "\n",
    "# Store all losses here\n",
    "losses_pf = []\n",
    "\n",
    "# Use a temperature for the objective function\n",
    "t = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Put a zero into losses. This is where we will cumulatively sum all the losses\n",
    "    # from each batch. After all batches are done, we will divide by the number of batches\n",
    "    # to obtain the average loss per batch\n",
    "    losses_pf.append(0)\n",
    "\n",
    "    # To compute the number of batches (since it varies depending on dataset size)\n",
    "    # update a counter variable\n",
    "    number_of_batches = 0\n",
    "\n",
    "    # Grab the batch, we are only interested in images not on their labels\n",
    "    for images, _ in trainloader:\n",
    "        beta = 1.0 #min(1, 0.01 + t / 700)\n",
    "        # Store image batch to device\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Set previous gradients to zero\n",
    "        optimizer_pf.zero_grad()\n",
    "\n",
    "        # Feed images through the VAE to obtain their reconstruction\n",
    "        recon_pf, log_q0, ladj_sum, log_pK = vae_pf(images)\n",
    "\n",
    "\n",
    "        # Compare reconstruction and images via the loss function\n",
    "        #print(\"-\"*80)\n",
    "        #print(\"Batch: \", number_of_batches)\n",
    "        #print(\"Training Loop\")\n",
    "        #print(\"images size: \", images.size())\n",
    "        #print(\"recon_pf size: \", recon_pf.size())\n",
    "        #print(\"kl_pf size: \", kl_pf.size())\n",
    "        BCE = F.binary_cross_entropy(recon_pf.view(-1, 784), images.view(-1, 784), reduction='sum')\n",
    "        #print(\"log_qK: \", log_qK.sum())\n",
    "        #print(\"log_pK: \", log_pK.sum())\n",
    "        #print(\"log_q0: \", log_q0.sum())\n",
    "        #print(\"ladj  : \", ladj_sum.sum())\n",
    "        KL  = (log_q0 + ladj_sum - beta*log_pK).sum()\n",
    "        #print(\"BCE : \", BCE)\n",
    "        #print(\"KL  : \", KL)\n",
    "        #print(\"beta: \", beta)\n",
    "        loss_pf = beta*BCE + KL\n",
    "        #print(\"loss_pf: \", loss_pf)\n",
    "        #print(\"type(loss_pf): \", type(loss_pf))\n",
    "\n",
    "        # Backpropagate the loss. Before doing that, make sure all previously stored gradients are zero (e.g. from previous iterations)\n",
    "        loss_pf.backward()\n",
    "\n",
    "        # Use the accumulated gradients to do a step in the right direction\n",
    "        optimizer_pf.step()\n",
    "\n",
    "        # Add loss to the cumulative sum\n",
    "        losses_pf[-1] += loss_pf.item()        #.item() grabs the value in a tensor with only 1 value\n",
    "        number_of_batches += 1\n",
    "        t += 1\n",
    "\n",
    "    # At the end of all batches divide the total loss for this epoch by the number of \n",
    "    # batches to obtain an average loss per batch\n",
    "    losses_pf[-1] /= number_of_batches\n",
    "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, epochs, losses_pf[-1]))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(losses_pf)\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend([\"VAE+PF\", \"Vanilla VAE\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
